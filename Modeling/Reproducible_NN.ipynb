{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ec943de-4136-44ec-9356-08ca018e66ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import sys\n",
    "import re\n",
    "import sklearn.preprocessing\n",
    "import warnings\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Suppress TensorFlow performance warnings and future warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c4c1157-8220-43c0-b3eb-c97b1b7d7fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "DATASET_FILE = 'final_ml_features_normalized.csv' \n",
    "TARGET_COLUMN = 'Median_Housing_Value_2023' \n",
    "RANDOM_STATE = 1738\n",
    "TRAIN_SIZE = 0.7  \n",
    "VAL_SIZE = 0.15   \n",
    "TEST_SIZE = 0.15  \n",
    "\n",
    "# =========================================================================\n",
    "# COMPLETE REPRODUCIBILITY SETUP\n",
    "# =========================================================================\n",
    "\n",
    "def set_random_seeds(seed=1738):\n",
    "    \"\"\"Set all random seeds for complete reproducibility\"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "set_random_seeds(RANDOM_STATE)\n",
    "\n",
    "# 1. BASE FEATURE DEFINITION (Lagged and Pruned for Leakage) - 16 columns\n",
    "BASE_NON_VIBE_COLS = [\n",
    "    'Total_Population_2021', 'Median_Household_Income_2021', \n",
    "    'Owner_Occupied_Units_2021', 'Bachelors_Degree_Count_2021', 'Masters_Degree_Count_2021', \n",
    "    'Unemployed_Count_2021', 'Unemployment_Rate_2021', 'Bachelors_Or_Higher_Rate_2021', \n",
    "    'Total_Population_2022', 'Median_Household_Income_2022', \n",
    "    'Owner_Occupied_Units_2022', 'Bachelors_Degree_Count_2022', 'Masters_Degree_Count_2022', \n",
    "    'Unemployed_Count_2022', 'Unemployment_Rate_2022', 'Bachelors_Or_Higher_Rate_2022'\n",
    "]\n",
    "\n",
    "# 2. VIBE FEATURE DEFINITION (Full list)\n",
    "VIBE_COLS_ALL = [\n",
    "    'fortune', 'sbs', 'count', \n",
    "    'water_0', 'water_beach', 'water_coastline', 'water_harbor_harbour', 'water_lake', \n",
    "    'water_marina', 'water_river',\n",
    "    'n_restaurants_PER_10K', 'n_cuisine_PER_10K', 'n_cuisine_mexican_PER_10K', \n",
    "    'n_cuisine_pizza_PER_10K', 'n_cuisine_american_PER_10K', 'n_cuisine_japanese_PER_10K', \n",
    "    'n_cuisine_thai_PER_10K', 'n_cuisine_sushi_PER_10K', 'n_cuisine_italian_PER_10K', \n",
    "    'n_cuisine_chinese_PER_10K', 'n_cuisine_korean_PER_10K', 'n_cuisine_burger_PER_10K', \n",
    "    'n_cuisine_mediterranean_PER_10K', 'n_cuisine_breakfast_PER_10K', 'n_cuisine_seafood_PER_10K', \n",
    "    'n_cuisine_asian_PER_10K', 'n_cuisine_indian_PER_10K', 'n_cuisine_barbecue_PER_10K', \n",
    "    'n_cuisine_sandwich_PER_10K', 'n_cuisine_steak_house_PER_10K', 'n_cuisine_vietnamese_PER_10K', \n",
    "    'n_cuisine_noodle_PER_10K', 'n_cuisine_chicken_PER_10K', 'n_cuisine_ramen_PER_10K', \n",
    "    'n_cuisine_french_PER_10K', 'n_cuisine_pasta_PER_10K', 'n_cuisine_filipino_PER_10K', \n",
    "    'n_cuisine_coffee_shop_PER_10K', 'n_cuisine_salad_PER_10K', 'n_cuisine_pancake_PER_10K', \n",
    "    'n_cuisine_peruvian_PER_10K', 'n_cuisine_hawaiian_PER_10K', 'n_cuisine_greek_PER_10K', \n",
    "    'n_cuisine_regional_PER_10K', 'n_cuisine_poke_PER_10K', 'n_cuisine_lebanese_PER_10K', \n",
    "    'n_cuisine_kebab_PER_10K', 'n_cuisine_fish_PER_10K', 'n_cuisine_spanish_PER_10K', \n",
    "    'n_cuisine_diner_PER_10K', 'n_cuisine_tacos_PER_10K', 'n_cuisine_dessert_PER_10K', \n",
    "    'n_cuisine_deli_PER_10K', 'n_cuisine_ice_cream_PER_10K', 'n_cuisine_brazilian_PER_10K', \n",
    "    'n_cuisine_middle_eastern_PER_10K', 'n_cuisine_tex-mex_PER_10K', 'n_cuisine_lunch_PER_10K', \n",
    "    'n_cuisine_grill_PER_10K', 'n_cuisine_brunch_PER_10K', 'n_cuisine_southern_PER_10K', \n",
    "    'n_cuisine_italian_pizza_PER_10K', 'n_cuisine_latin_american_PER_10K', 'n_cuisine_wings_PER_10K', \n",
    "    'n_cuisine_german_PER_10K', 'n_cuisine_steak_PER_10K', 'n_cuisine_bagel_PER_10K', \n",
    "    'n_cuisine_tapas_PER_10K', 'n_cuisine_fish_and_chips_PER_10K', 'n_cuisine_international_PER_10K', \n",
    "    'n_cuisine_jamaican_PER_10K', 'n_cuisine_turkish_PER_10K', 'n_cuisine_cajun_PER_10K', \n",
    "    'n_cuisine_soup_PER_10K', 'n_cuisine_buffet_PER_10K', 'n_cuisine_donut_PER_10K', \n",
    "    'n_cuisine_mongolian_grill_PER_10K', 'n_cuisine_caribbean_PER_10K', 'n_cuisine_hot_dog_PER_10K', \n",
    "    'n_cuisine_bar_and_grill_PER_10K', 'n_venues_PER_10K', 'n_venue_type_PER_10K', \n",
    "    'n_venue_bar_PER_10K', 'n_venue_fitness_centre_PER_10K', 'n_venue_theatre_PER_10K', \n",
    "    'n_venue_sports_centre_PER_10K', 'n_venue_cinema_PER_10K', 'n_venue_public_bookcase_PER_10K', \n",
    "    'n_venue_pub_PER_10K', 'n_venue_nightclub_PER_10K', 'n_venue_arts_centre_PER_10K', \n",
    "    'n_venue_stadium_PER_10K', 'n_venue_public_building_PER_10K', 'n_venue_casino_PER_10K', \n",
    "    'n_cafe_PER_10K', 'n_cafe_cuisine_PER_10K', 'n_cafe_brand_PER_10K', \n",
    "    'n_cafe_brand_Starbucks_PER_10K', 'n_cafe_brand_Dunkin\\'_PER_10K', 'n_cafe_amenity_cafe_PER_10K', \n",
    "    'n_cafe_amenity_fast_food_PER_10K', 'n_cafe_cuisine_coffee_shop_PER_10K', 'n_cafe_cuisine_bubble_tea_PER_10K', \n",
    "    'n_cafe_cuisine_donut_PER_10K', 'n_cafe_cuisine_sandwich_PER_10K', 'n_cafe_cuisine_breakfast_PER_10K', \n",
    "    'n_cafe_cuisine_american_PER_10K', 'n_cafe_cuisine_coffee_PER_10K', 'n_cafe_cuisine_pastry_PER_10K', \n",
    "    'n_cafe_cuisine_tea_PER_10K', 'n_cafe_cuisine_dessert_PER_10K', 'n_cafe_cuisine_bagel_PER_10K', \n",
    "    'n_cafe_cuisine_ice_cream_PER_10K', 'n_cafe_brand_Tim_Hortons_PER_10K', 'n_cafe_brand_Dutch_Bros._Coffee_PER_10K', \n",
    "    'amenity_arts_centre_PER_10K', 'amenity_bar_PER_10K', 'amenity_community_centre_PER_10K', \n",
    "    'amenity_concert_hall_PER_10K', 'amenity_music_venue_PER_10K', 'amenity_nightclub_PER_10K', \n",
    "    'amenity_parking_PER_10K', 'amenity_pub_PER_10K', 'amenity_school_PER_10K', \n",
    "    'amenity_theatre_PER_10K', 'leisure_park_PER_10K', 'leisure_stadium_PER_10K', \n",
    "    'tourism_artwork_PER_10K', 'tourism_attraction_PER_10K', 'tourism_gallery_PER_10K', \n",
    "    'tourism_museum_PER_10K', 'tourism_picnic_site_PER_10K', 'tourism_viewpoint_PER_10K', \n",
    "    'tourism_yes_PER_10K', 'arts_theatre_events_PER_10K', 'music_events_PER_10K', \n",
    "    'other_events_PER_10K', 'sports_events_PER_10K', 'arts_theatre_venues_PER_10K', \n",
    "    'music_venues_PER_10K', 'other_venues_PER_10K', 'sports_venues_PER_10K'\n",
    "]\n",
    "\n",
    "# 3. COMBINED VIBE FEATURE SET\n",
    "FULL_VIBE_COLS = BASE_NON_VIBE_COLS + VIBE_COLS_ALL\n",
    "\n",
    "# Define Categorical Features\n",
    "CATEGORICAL_COLS = [\n",
    "    'water_0', 'water_beach', 'water_coastline', 'water_harbor_harbour', \n",
    "    'water_lake', 'water_marina', 'water_river'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bfbc04d-679b-4ef5-9687-b506c9a1027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================\n",
    "# 1. DATA PREPARATION AND SPLITTING\n",
    "# =========================================================================\n",
    "\n",
    "def load_and_split_data(file_path, target_col, feature_cols_subset, global_cat_cols, train_size, val_size, test_size):\n",
    "    \"\"\"Loads data, filters duplicates, filters NaNs, and splits based on a specific feature subset.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Data file not found at {file_path}. Please check path.\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    # --- 1a. Initial Filtering and Target/Population Check ---\n",
    "    merge_key_col = [col for col in df.columns if 'Merge_Key' in col][0]\n",
    "    pop_col = [col for col in df.columns if re.search(r'Total_Population', col, re.IGNORECASE)][0]\n",
    "    df[pop_col] = pd.to_numeric(df[pop_col], errors='coerce')\n",
    "    \n",
    "    # --- 1b. Handle Duplicate Cities by Max Population ---\n",
    "    initial_len = len(df)\n",
    "    df = df.sort_values(by=pop_col, ascending=False).drop_duplicates(subset=[merge_key_col], keep='first').copy()\n",
    "    print(f\"Duplicate City Entries Removed: {initial_len - len(df)} rows dropped.\")\n",
    "    \n",
    "    # --- 1c. Feature/Target Separation and NaN Target Removal ---\n",
    "    required_cols = feature_cols_subset + [target_col]\n",
    "    df_filtered = df[df.columns.intersection(required_cols)].copy()\n",
    "    \n",
    "    # Filter rows missing the target value\n",
    "    df_filtered = df_filtered.dropna(subset=[target_col])\n",
    "    \n",
    "    X = df_filtered.drop(target_col, axis=1, errors='ignore')\n",
    "    y = df_filtered[target_col]\n",
    "    \n",
    "    # --- 1d. DYNAMIC FEATURE IDENTIFICATION ---\n",
    "    \n",
    "    # 1. Identify which categorical columns are actually present in this model's X subset\n",
    "    actual_cat_cols = [col for col in global_cat_cols if col in X.columns]\n",
    "    \n",
    "    # 2. Identify numerical columns present in this subset\n",
    "    actual_num_cols = [col for col in X.columns if col not in actual_cat_cols and X[col].dtype != 'object']\n",
    "    \n",
    "    # 3. Final Imputation\n",
    "    for col in actual_num_cols:\n",
    "        X[col] = X[col].fillna(X[col].median())\n",
    "    for col in actual_cat_cols:\n",
    "        X[col] = X[col].fillna('missing') \n",
    "    \n",
    "    # Ensure X only contains the columns used in the preprocessor\n",
    "    X = X[actual_num_cols + actual_cat_cols]\n",
    "\n",
    "    # --- 1e. Final Split ---\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    relative_val_size = val_size / (train_size + val_size)\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=relative_val_size, random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    print(f\"Data Loaded and Split (Actual Features: {len(actual_num_cols) + len(actual_cat_cols)}):\")\n",
    "    print(f\"  Training Set Size: {len(X_train)}\")\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, actual_num_cols, actual_cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff3049e4-9f30-41b5-97db-ee1473d7650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================\n",
    "# 2. PREPROCESSOR DEFINITION \n",
    "# =========================================================================\n",
    "\n",
    "def define_preprocessor(numerical_features, categorical_features):\n",
    "    \"\"\"Defines the ColumnTransformer pipeline for scaling and encoding.\"\"\"\n",
    "    \n",
    "    numerical_pipeline = StandardScaler()\n",
    "    categorical_pipeline = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_pipeline, numerical_features),\n",
    "            ('cat', categorical_pipeline, categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "    return preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47e8b76b-fb3f-4336-9c9e-fb2a4e2f5db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================\n",
    "# 3. NEURAL NETWORK MODELING (TensorFlow/Keras)\n",
    "# =========================================================================\n",
    "\n",
    "def build_mlp_model(input_shape, hidden_layers, activation='relu', dropout_rate=0.2, learning_rate=0.001):\n",
    "    \"\"\"Creates a sequential Multi-Layer Perceptron (MLP) model for regression.\"\"\"\n",
    "    # Reset seeds before building each model\n",
    "    set_random_seeds(RANDOM_STATE)\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # Input Layer and First Hidden Layer\n",
    "    model.add(Dense(hidden_layers[0], activation=activation, input_shape=(input_shape,),\n",
    "                    kernel_initializer=tf.keras.initializers.glorot_uniform(seed=RANDOM_STATE),\n",
    "                    bias_initializer='zeros'))\n",
    "    model.add(Dropout(dropout_rate, seed=RANDOM_STATE))\n",
    "    \n",
    "    # Additional Hidden Layers\n",
    "    for units in hidden_layers[1:]:\n",
    "        model.add(Dense(units, activation=activation,\n",
    "                       kernel_initializer=tf.keras.initializers.glorot_uniform(seed=RANDOM_STATE),\n",
    "                       bias_initializer='zeros'))\n",
    "        model.add(Dropout(dropout_rate, seed=RANDOM_STATE))\n",
    "        \n",
    "    # Output Layer\n",
    "    model.add(Dense(1,\n",
    "                   kernel_initializer=tf.keras.initializers.glorot_uniform(seed=RANDOM_STATE),\n",
    "                   bias_initializer='zeros')) \n",
    "    \n",
    "    # Compile the model with fixed optimizer\n",
    "    optimizer = tf.keras.optimizers.legacy.Adam(\n",
    "        learning_rate=learning_rate,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-07,\n",
    "        amsgrad=False\n",
    "    )\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae', tf.keras.metrics.R2Score()])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def run_hyperparameter_search(X_train, y_train, X_val, y_val, num_features, model_name):\n",
    "    \"\"\"Performs a custom grid search over predefined NN architecture and hyperparameters.\"\"\"\n",
    "    \n",
    "    # Define the search space\n",
    "    layer_configs = [(256, 128, 64), (128, 64), (64, 32), (256, 128)]\n",
    "    learning_rates = [0.01, 0.001, 0.0001]\n",
    "    batch_sizes = [32, 64, 128]\n",
    "    dropout_rates = [0.1, 0.3]\n",
    "    \n",
    "    best_r2 = -float('inf')\n",
    "    best_params = {}\n",
    "    best_model = None\n",
    "    \n",
    "    print(f\"\\n--- Starting NN Grid Search for {model_name} ---\")\n",
    "    \n",
    "    # Callbacks for better training\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "    total_runs = len(layer_configs) * len(learning_rates) * len(batch_sizes) * len(dropout_rates)\n",
    "    run_count = 0\n",
    "    \n",
    "    for layers in layer_configs:\n",
    "        for lr in learning_rates:\n",
    "            for bs in batch_sizes:\n",
    "                for drop in dropout_rates:\n",
    "                    run_count += 1\n",
    "                    \n",
    "                    # Reset everything for each model\n",
    "                    tf.keras.backend.clear_session()\n",
    "                    set_random_seeds(RANDOM_STATE)\n",
    "                    \n",
    "                    # 1. Build Model\n",
    "                    model = build_mlp_model(input_shape=num_features, hidden_layers=layers, learning_rate=lr, dropout_rate=drop)\n",
    "                    \n",
    "                    # 2. Train Model\n",
    "                    history = model.fit(\n",
    "                        X_train, y_train, validation_data=(X_val, y_val), \n",
    "                        epochs=100, batch_size=bs, verbose=0, \n",
    "                        callbacks=[early_stopping, reduce_lr],\n",
    "                        shuffle=False\n",
    "                    )\n",
    "\n",
    "                    # 3. Evaluate on Validation Set\n",
    "                    val_metrics = model.evaluate(X_val, y_val, verbose=0)\n",
    "                    val_r2 = val_metrics[-1]\n",
    "                    \n",
    "                    # 4. Track Best Model\n",
    "                    if val_r2 > best_r2:\n",
    "                        best_r2 = val_r2\n",
    "                        best_params = {'hidden_layer_sizes': layers, 'learning_rate': lr, 'batch_size': bs, 'dropout_rate': drop}\n",
    "                        best_model = model \n",
    "\n",
    "    print(\"\\n--- Grid Search Complete ---\")\n",
    "    return best_model, best_params, best_r2\n",
    "\n",
    "def evaluate_test_predictions(y_test_pred_scaled, y_test_true_scaled, y_scaler):\n",
    "    \"\"\"Calculates and returns standard regression metrics after inverse-transforming predictions.\"\"\"\n",
    "    \n",
    "    # Inverse transform predictions and true values back to original dollar scale\n",
    "    y_pred = y_scaler.inverse_transform(y_test_pred_scaled).flatten()\n",
    "    y_true = y_scaler.inverse_transform(y_test_true_scaled).flatten() \n",
    "\n",
    "    if y_pred.shape != y_true.shape:\n",
    "        print(f\"ERROR: Prediction shape ({y_pred.shape}) does not match true target shape ({y_true.shape}). Cannot evaluate.\")\n",
    "        return {'RMSE': np.nan, 'MAE': np.nan, 'R2': np.nan}\n",
    "\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    return {'RMSE': np.sqrt(mse), 'MAE': mae, 'R2': r2}\n",
    "\n",
    "def print_evaluation_summary(model_name, metrics, best_params):\n",
    "    \"\"\"Formats and prints the final model results.\"\"\"\n",
    "    print(f\"\\n--- {model_name} Test Set Evaluation ---\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Optimal Architecture: {best_params['hidden_layer_sizes']}, LR={best_params['learning_rate']}, Drop={best_params['dropout_rate']}\")\n",
    "    print(f\"Metric:              Value (in Original Scale)\")\n",
    "    print(f\"--------------------------------------------------\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {metrics['RMSE']:.2f}\") \n",
    "    print(f\"Mean Absolute Error (MAE):     {metrics['MAE']:.2f}\")\n",
    "    print(f\"Coefficient of Determination (R²): {metrics['R2']:.4f}\")\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "876560fc-18be-49cd-87a3-8cb0b6f0bad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=======================================================\n",
      "MODEL RUN: NON-VIBE BASE MODEL\n",
      "=======================================================\n",
      "Duplicate City Entries Removed: 46 rows dropped.\n",
      "Data Loaded and Split (Actual Features: 16):\n",
      "  Training Set Size: 605\n",
      "Fitting and transforming features (Numeric: 16, Categorical: 0)...\n",
      "Features (Numeric+Encoded): 16\n",
      "\n",
      "--- Starting NN Grid Search for NON-VIBE BASE MODEL ---\n",
      "\n",
      "--- Grid Search Complete ---\n",
      "\n",
      "--- NON-VIBE BASE MODEL Test Set Evaluation ---\n",
      "==================================================\n",
      "Optimal Architecture: (256, 128, 64), LR=0.01, Drop=0.1\n",
      "Metric:              Value (in Original Scale)\n",
      "--------------------------------------------------\n",
      "Root Mean Squared Error (RMSE): 143621.49\n",
      "Mean Absolute Error (MAE):     106303.38\n",
      "Coefficient of Determination (R²): 0.7047\n",
      "==================================================\n",
      "\n",
      "\n",
      "=======================================================\n",
      "MODEL RUN: FULL VIBE MODEL\n",
      "=======================================================\n",
      "Duplicate City Entries Removed: 46 rows dropped.\n",
      "Data Loaded and Split (Actual Features: 158):\n",
      "  Training Set Size: 605\n",
      "Fitting and transforming features (Numeric: 151, Categorical: 7)...\n",
      "Features (Numeric+Encoded): 164\n",
      "\n",
      "--- Starting NN Grid Search for FULL VIBE MODEL ---\n",
      "\n",
      "--- Grid Search Complete ---\n",
      "\n",
      "--- FULL VIBE MODEL Test Set Evaluation ---\n",
      "==================================================\n",
      "Optimal Architecture: (256, 128), LR=0.01, Drop=0.1\n",
      "Metric:              Value (in Original Scale)\n",
      "--------------------------------------------------\n",
      "Root Mean Squared Error (RMSE): 162301.34\n",
      "Mean Absolute Error (MAE):     109146.88\n",
      "Coefficient of Determination (R²): 0.6229\n",
      "==================================================\n",
      "\n",
      "\n",
      "######################################################\n",
      "####### FINAL VIBE FEATURE IMPACT COMPARISON #########\n",
      "######################################################\n",
      "CONCLUSION: VIBE Features did NOT significantly improve R².\n",
      "Baseline R² (Non-VIBE): 0.7047\n",
      "VIBE Model R²:          0.6229\n",
      "\n",
      "RMSE Comparison:\n",
      "Baseline RMSE: $143621.49\n",
      "VIBE Model RMSE: $162301.34\n"
     ]
    }
   ],
   "source": [
    "# =========================================================================\n",
    "# 4. EXECUTION FLOW\n",
    "# =========================================================================\n",
    "\n",
    "def execute_model_run(feature_subset, model_name):\n",
    "    \"\"\"Executes the full preprocessing, training, and evaluation for a given feature subset.\"\"\"\n",
    "    \n",
    "    print(f\"\\n\\n=======================================================\")\n",
    "    print(f\"MODEL RUN: {model_name}\")\n",
    "    print(f\"=======================================================\")\n",
    "    \n",
    "    # --- 1. Load and Split Data ---\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test, actual_num_cols, actual_cat_cols = load_and_split_data(\n",
    "        DATASET_FILE, TARGET_COLUMN, feature_subset, CATEGORICAL_COLS, TRAIN_SIZE, VAL_SIZE, TEST_SIZE\n",
    "    )\n",
    "    \n",
    "    # --- 2. Target Scaling ---\n",
    "    y_scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "    y_train_scaled = y_scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
    "    y_val_scaled = y_scaler.transform(y_val.values.reshape(-1, 1))\n",
    "    y_test_scaled = y_scaler.transform(y_test.values.reshape(-1, 1))\n",
    "    \n",
    "    # --- 3. Preprocessing Pipeline ---\n",
    "    preprocessor = define_preprocessor(actual_num_cols, actual_cat_cols)\n",
    "    \n",
    "    print(f\"Fitting and transforming features (Numeric: {len(actual_num_cols)}, Categorical: {len(actual_cat_cols)})...\")\n",
    "    X_train_proc = preprocessor.fit_transform(X_train)\n",
    "    X_val_proc = preprocessor.transform(X_val)\n",
    "    X_test_proc = preprocessor.transform(X_test)\n",
    "    \n",
    "    # --- 4. Type Conversion for TensorFlow ---\n",
    "    X_train_proc = X_train_proc.astype(np.float32)\n",
    "    X_val_proc = X_val_proc.astype(np.float32)\n",
    "    X_test_proc = X_test_proc.astype(np.float32)\n",
    "    \n",
    "    y_train_proc = y_train_scaled.astype(np.float32)\n",
    "    y_val_proc = y_val_scaled.astype(np.float32)\n",
    "    y_test_proc = y_test_scaled.astype(np.float32) \n",
    "    \n",
    "    num_features = X_train_proc.shape[1]\n",
    "    print(f\"Features (Numeric+Encoded): {num_features}\")\n",
    "    \n",
    "    # --- 5. Hyperparameter Search and Training ---\n",
    "    best_model, best_params, best_val_r2 = run_hyperparameter_search(\n",
    "        X_train_proc, y_train_proc, X_val_proc, y_val_proc, num_features, model_name\n",
    "    )\n",
    "\n",
    "    # --- 6. Final Evaluation on Test Set ---\n",
    "    if best_model:\n",
    "        y_test_pred_scaled = best_model.predict(X_test_proc, verbose=0)\n",
    "        \n",
    "        test_metrics = evaluate_test_predictions(y_test_pred_scaled, y_test_proc, y_scaler)\n",
    "        \n",
    "        print_evaluation_summary(model_name, test_metrics, best_params)\n",
    "        \n",
    "        return best_model, best_params, test_metrics\n",
    "    else:\n",
    "        print(f\"ERROR: {model_name} failed to find a best model.\")\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run both Non-VIBE and VIBE models and compare results.\"\"\"\n",
    "    \n",
    "    set_random_seeds(RANDOM_STATE)\n",
    "    \n",
    "    # --- 1. RUN NON-VIBE BASE MODEL ---\n",
    "    # Enforce lagging: only use BASE_NON_VIBE_COLS\n",
    "    \n",
    "    model_non_vibe, params_non_vibe, metrics_non_vibe = execute_model_run(\n",
    "        BASE_NON_VIBE_COLS, \"NON-VIBE BASE MODEL\"\n",
    "    )\n",
    "    \n",
    "    # --- 2. RUN FULL VIBE MODEL ---\n",
    "    # Use all features: BASE_NON_VIBE_COLS + VIBE_COLS\n",
    "    \n",
    "    model_vibe, params_vibe, metrics_vibe = execute_model_run(\n",
    "        FULL_VIBE_COLS, \"FULL VIBE MODEL\"\n",
    "    )\n",
    "    \n",
    "    # --- 3. FINAL PROJECT COMPARISON ---\n",
    "    \n",
    "    if metrics_non_vibe and metrics_vibe:\n",
    "        print(\"\\n\\n######################################################\")\n",
    "        print(\"####### FINAL VIBE FEATURE IMPACT COMPARISON #########\")\n",
    "        print(\"######################################################\")\n",
    "        \n",
    "        r2_non_vibe = metrics_non_vibe['R2']\n",
    "        r2_vibe = metrics_vibe['R2']\n",
    "        \n",
    "        if r2_vibe > r2_non_vibe:\n",
    "            improvement = (r2_vibe - r2_non_vibe) / r2_non_vibe\n",
    "            print(f\"CONCLUSION: VIBE Features IMPROVED Predictive Power.\")\n",
    "            print(f\"Baseline R² (Non-VIBE): {r2_non_vibe:.4f}\")\n",
    "            print(f\"VIBE Model R²:          {r2_vibe:.4f} (+{improvement:.1%})\")\n",
    "        else:\n",
    "            print(f\"CONCLUSION: VIBE Features did NOT significantly improve R².\")\n",
    "            print(f\"Baseline R² (Non-VIBE): {r2_non_vibe:.4f}\")\n",
    "            print(f\"VIBE Model R²:          {r2_vibe:.4f}\")\n",
    "            \n",
    "        print(\"\\nRMSE Comparison:\")\n",
    "        print(f\"Baseline RMSE: ${metrics_non_vibe['RMSE']:.2f}\")\n",
    "        print(f\"VIBE Model RMSE: ${metrics_vibe['RMSE']:.2f}\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tf.keras.backend.clear_session()\n",
    "    set_random_seeds(RANDOM_STATE)\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf215)",
   "language": "python",
   "name": "tf215"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
