{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55126993-876f-4d3e-8efc-19eca70e6849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Duplicates Removed: 46 rows dropped (Base Data).\n",
      "\n",
      "\n",
      "=======================================================\n",
      "MODEL RUN: NON-VIBE BASE MODEL\n",
      "=======================================================\n",
      "Data Loaded and Split (Actual Features: 17):\n",
      "  Training Set Size: 605\n",
      "Fitting and transforming features (Numeric: 16, Categorical: 1)...\n",
      "Features (Numeric+Encoded): 20\n",
      "\n",
      "--- Starting NN Grid Search for NON-VIBE BASE MODEL ---\n",
      "\n",
      "--- Grid Search Complete ---\n",
      "\n",
      "--- NON-VIBE BASE MODEL Test Set Evaluation ---\n",
      "==================================================\n",
      "Optimal Architecture: (128, 64), LR=0.01, Drop=0.3\n",
      "Metric:              Value (in Original Dollar Scale)\n",
      "--------------------------------------------------\n",
      "Root Mean Squared Error (RMSE): 124967.76\n",
      "Mean Absolute Error (MAE):     84919.14\n",
      "Coefficient of Determination (R²): 0.7765\n",
      "==================================================\n",
      "\n",
      "\n",
      "=======================================================\n",
      "MODEL RUN: FULL VIBE MODEL\n",
      "=======================================================\n",
      "Data Loaded and Split (Actual Features: 159):\n",
      "  Training Set Size: 605\n",
      "Fitting and transforming features (Numeric: 151, Categorical: 8)...\n",
      "Features (Numeric+Encoded): 168\n",
      "\n",
      "--- Starting NN Grid Search for FULL VIBE MODEL ---\n",
      "\n",
      "--- Grid Search Complete ---\n",
      "\n",
      "--- FULL VIBE MODEL Test Set Evaluation ---\n",
      "==================================================\n",
      "Optimal Architecture: (256, 128, 64), LR=0.01, Drop=0.1\n",
      "Metric:              Value (in Original Dollar Scale)\n",
      "--------------------------------------------------\n",
      "Root Mean Squared Error (RMSE): 123918.50\n",
      "Mean Absolute Error (MAE):     93156.88\n",
      "Coefficient of Determination (R²): 0.7802\n",
      "==================================================\n",
      "\n",
      "\n",
      "######################################################\n",
      "####### FINAL VIBE FEATURE IMPACT COMPARISON #########\n",
      "######################################################\n",
      "CONCLUSION: VIBE Features IMPROVED Predictive Power.\n",
      "Baseline R² (Non-VIBE): 0.7765\n",
      "VIBE Model R²:          0.7802 (+0.5%)\n",
      "\n",
      "RMSE Comparison (in Dollars):\n",
      "Baseline RMSE: $124967.76\n",
      "VIBE Model RMSE: $123918.50\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import time\n",
    "import sys\n",
    "import re\n",
    "import sklearn.preprocessing\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "DATASET_FILE = 'final_ml_features_normalized.csv' \n",
    "TARGET_COLUMN = 'Median_Housing_Value_2023' \n",
    "RANDOM_STATE = 1738\n",
    "TRAIN_SIZE = 0.7  \n",
    "VAL_SIZE = 0.15   \n",
    "TEST_SIZE = 0.15  \n",
    "\n",
    "# =========================================================================\n",
    "# COMPLETE REPRODUCIBILITY SETUP\n",
    "# =========================================================================\n",
    "\n",
    "def set_random_seeds(seed=1738):\n",
    "    \"\"\"Set all random seeds for complete reproducibility\"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "set_random_seeds(RANDOM_STATE)\n",
    "\n",
    "# 1. BASE FEATURE DEFINITION (Lagged and Pruned for Leakage) - 16 columns\n",
    "BASE_NON_VIBE_COLS = [\n",
    "    'Total_Population_2021', 'Median_Household_Income_2021', \n",
    "    'Owner_Occupied_Units_2021', 'Bachelors_Degree_Count_2021', 'Masters_Degree_Count_2021', \n",
    "    'Unemployed_Count_2021', 'Unemployment_Rate_2021', 'Bachelors_Or_Higher_Rate_2021', \n",
    "    'Total_Population_2022', 'Median_Household_Income_2022', \n",
    "    'Owner_Occupied_Units_2022', 'Bachelors_Degree_Count_2022', 'Masters_Degree_Count_2022', \n",
    "    'Unemployed_Count_2022', 'Unemployment_Rate_2022', 'Bachelors_Or_Higher_Rate_2022'\n",
    "]\n",
    "\n",
    "# 2. VIBE FEATURE DEFINITION (Full list)\n",
    "VIBE_COLS_ALL = [\n",
    "    'fortune', 'sbs', 'count', \n",
    "    'water_0', 'water_beach', 'water_coastline', 'water_harbor_harbour', 'water_lake', \n",
    "    'water_marina', 'water_river',\n",
    "    'n_restaurants_PER_10K', 'n_cuisine_PER_10K', 'n_cuisine_mexican_PER_10K', \n",
    "    'n_cuisine_pizza_PER_10K', 'n_cuisine_american_PER_10K', 'n_cuisine_japanese_PER_10K', \n",
    "    'n_cuisine_thai_PER_10K', 'n_cuisine_sushi_PER_10K', 'n_cuisine_italian_PER_10K', \n",
    "    'n_cuisine_chinese_PER_10K', 'n_cuisine_korean_PER_10K', 'n_cuisine_burger_PER_10K', \n",
    "    'n_cuisine_mediterranean_PER_10K', 'n_cuisine_breakfast_PER_10K', 'n_cuisine_seafood_PER_10K', \n",
    "    'n_cuisine_asian_PER_10K', 'n_cuisine_indian_PER_10K', 'n_cuisine_barbecue_PER_10K', \n",
    "    'n_cuisine_sandwich_PER_10K', 'n_cuisine_steak_house_PER_10K', 'n_cuisine_vietnamese_PER_10K', \n",
    "    'n_cuisine_noodle_PER_10K', 'n_cuisine_chicken_PER_10K', 'n_cuisine_ramen_PER_10K', \n",
    "    'n_cuisine_french_PER_10K', 'n_cuisine_pasta_PER_10K', 'n_cuisine_filipino_PER_10K', \n",
    "    'n_cuisine_coffee_shop_PER_10K', 'n_cuisine_salad_PER_10K', 'n_cuisine_pancake_PER_10K', \n",
    "    'n_cuisine_peruvian_PER_10K', 'n_cuisine_hawaiian_PER_10K', 'n_cuisine_greek_PER_10K', \n",
    "    'n_cuisine_regional_PER_10K', 'n_cuisine_poke_PER_10K', 'n_cuisine_lebanese_PER_10K', \n",
    "    'n_cuisine_kebab_PER_10K', 'n_cuisine_fish_PER_10K', 'n_cuisine_spanish_PER_10K', \n",
    "    'n_cuisine_diner_PER_10K', 'n_cuisine_tacos_PER_10K', 'n_cuisine_dessert_PER_10K', \n",
    "    'n_cuisine_deli_PER_10K', 'n_cuisine_ice_cream_PER_10K', 'n_cuisine_brazilian_PER_10K', \n",
    "    'n_cuisine_middle_eastern_PER_10K', 'n_cuisine_tex-mex_PER_10K', 'n_cuisine_lunch_PER_10K', \n",
    "    'n_cuisine_grill_PER_10K', 'n_cuisine_brunch_PER_10K', 'n_cuisine_southern_PER_10K', \n",
    "    'n_cuisine_italian_pizza_PER_10K', 'n_cuisine_latin_american_PER_10K', 'n_cuisine_wings_PER_10K', \n",
    "    'n_cuisine_german_PER_10K', 'n_cuisine_steak_PER_10K', 'n_cuisine_bagel_PER_10K', \n",
    "    'n_cuisine_tapas_PER_10K', 'n_cuisine_fish_and_chips_PER_10K', 'n_cuisine_international_PER_10K', \n",
    "    'n_cuisine_jamaican_PER_10K', 'n_cuisine_turkish_PER_10K', 'n_cuisine_cajun_PER_10K', \n",
    "    'n_cuisine_soup_PER_10K', 'n_cuisine_buffet_PER_10K', 'n_cuisine_donut_PER_10K', \n",
    "    'n_cuisine_mongolian_grill_PER_10K', 'n_cuisine_caribbean_PER_10K', 'n_cuisine_hot_dog_PER_10K', \n",
    "    'n_cuisine_bar_and_grill_PER_10K', 'n_venues_PER_10K', 'n_venue_type_PER_10K', \n",
    "    'n_venue_bar_PER_10K', 'n_venue_fitness_centre_PER_10K', 'n_venue_theatre_PER_10K', \n",
    "    'n_venue_sports_centre_PER_10K', 'n_venue_cinema_PER_10K', 'n_venue_public_bookcase_PER_10K', \n",
    "    'n_venue_pub_PER_10K', 'n_venue_nightclub_PER_10K', 'n_venue_arts_centre_PER_10K', \n",
    "    'n_venue_stadium_PER_10K', 'n_venue_public_building_PER_10K', 'n_venue_casino_PER_10K', \n",
    "    'n_cafe_PER_10K', 'n_cafe_cuisine_PER_10K', 'n_cafe_brand_PER_10K', \n",
    "    'n_cafe_brand_Starbucks_PER_10K', 'n_cafe_brand_Dunkin\\'_PER_10K', 'n_cafe_amenity_cafe_PER_10K', \n",
    "    'n_cafe_amenity_fast_food_PER_10K', 'n_cafe_cuisine_coffee_shop_PER_10K', 'n_cafe_cuisine_bubble_tea_PER_10K', \n",
    "    'n_cafe_cuisine_donut_PER_10K', 'n_cafe_cuisine_sandwich_PER_10K', 'n_cafe_cuisine_breakfast_PER_10K', \n",
    "    'n_cafe_cuisine_american_PER_10K', 'n_cafe_cuisine_coffee_PER_10K', 'n_cafe_cuisine_pastry_PER_10K', \n",
    "    'n_cafe_cuisine_tea_PER_10K', 'n_cafe_cuisine_dessert_PER_10K', 'n_cafe_cuisine_bagel_PER_10K', \n",
    "    'n_cafe_cuisine_ice_cream_PER_10K', 'n_cafe_brand_Tim_Hortons_PER_10K', 'n_cafe_brand_Dutch_Bros._Coffee_PER_10K', \n",
    "    'amenity_arts_centre_PER_10K', 'amenity_bar_PER_10K', 'amenity_community_centre_PER_10K', \n",
    "    'amenity_concert_hall_PER_10K', 'amenity_music_venue_PER_10K', 'amenity_nightclub_PER_10K', \n",
    "    'amenity_parking_PER_10K', 'amenity_pub_PER_10K', 'amenity_school_PER_10K', \n",
    "    'amenity_theatre_PER_10K', 'leisure_park_PER_10K', 'leisure_stadium_PER_10K', \n",
    "    'tourism_artwork_PER_10K', 'tourism_attraction_PER_10K', 'tourism_gallery_PER_10K', \n",
    "    'tourism_museum_PER_10K', 'tourism_picnic_site_PER_10K', 'tourism_viewpoint_PER_10K', \n",
    "    'tourism_yes_PER_10K', 'arts_theatre_events_PER_10K', 'music_events_PER_10K', \n",
    "    'other_events_PER_10K', 'sports_events_PER_10K', 'arts_theatre_venues_PER_10K', \n",
    "    'music_venues_PER_10K', 'other_venues_PER_10K', 'sports_venues_PER_10K'\n",
    "]\n",
    "\n",
    "# 3. COMBINED VIBE FEATURE SET\n",
    "FULL_VIBE_COLS = BASE_NON_VIBE_COLS + VIBE_COLS_ALL\n",
    "\n",
    "# Define Categorical Features\n",
    "CATEGORICAL_COLS = [\n",
    "    'water_0', 'water_beach', 'water_coastline', 'water_harbor_harbour', \n",
    "    'water_lake', 'water_marina', 'water_river',\n",
    "    'region' # Added region to the categorical list\n",
    "]\n",
    "\n",
    "# =========================================================================\n",
    "# UTILITIES AND CORE FUNCTIONS\n",
    "# =========================================================================\n",
    "\n",
    "def set_random_seeds(seed=1738):\n",
    "    \"\"\"Set all random seeds for complete reproducibility\"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "def add_us_regions(df):\n",
    "    \"\"\"Derives the U.S. Census Region from the state abbreviation in the Merge_Key.\"\"\"\n",
    "    state_to_region = {\n",
    "        \"ME\": \"Northeast\", \"NH\": \"Northeast\", \"VT\": \"Northeast\", \"MA\": \"Northeast\",\n",
    "        \"RI\": \"Northeast\", \"CT\": \"Northeast\", \"NY\": \"Northeast\", \"NJ\": \"Northeast\",\n",
    "        \"PA\": \"Northeast\",\n",
    "        \"OH\": \"Midwest\", \"IN\": \"Midwest\", \"IL\": \"Midwest\", \"MI\": \"Midwest\",\n",
    "        \"WI\": \"Midwest\", \"MN\": \"Midwest\", \"IA\": \"Midwest\", \"MO\": \"Midwest\",\n",
    "        \"ND\": \"Midwest\", \"SD\": \"Midwest\", \"NE\": \"Midwest\", \"KS\": \"Midwest\",\n",
    "        \"DE\": \"South\", \"MD\": \"South\", \"DC\": \"South\", \"VA\": \"South\", \"WV\": \"South\",\n",
    "        \"NC\": \"South\", \"SC\": \"South\", \"GA\": \"South\", \"FL\": \"South\",\n",
    "        \"KY\": \"South\", \"TN\": \"South\", \"MS\": \"South\", \"AL\": \"South\",\n",
    "        \"OK\": \"South\", \"TX\": \"South\", \"AR\": \"South\", \"LA\": \"South\",\n",
    "        \"MT\": \"West\", \"ID\": \"West\", \"WY\": \"West\", \"CO\": \"West\", \"NM\": \"West\",\n",
    "        \"AZ\": \"West\", \"UT\": \"West\", \"NV\": \"West\", \"WA\": \"West\", \"OR\": \"West\",\n",
    "        \"CA\": \"West\", \"AK\": \"West\", \"HI\": \"West\",\n",
    "    }\n",
    "    \n",
    "    merge_key_col = [col for col in df.columns if 'Merge_Key' in col][0]\n",
    "    df['state_abbr'] = df[merge_key_col].str.split(',', expand=True)[1].str.strip().str.upper()\n",
    "    \n",
    "    df[\"region\"] = df[\"state_abbr\"].map(state_to_region).fillna(\"Other\")\n",
    "    df = df.drop(columns=['state_abbr'], errors='ignore')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_and_split_data(df_model, target_col, feature_cols, global_cat_cols, train_size, val_size, test_size):\n",
    "    \"\"\"Takes a processed DataFrame, filters, imputes, and splits.\"\"\"\n",
    "    \n",
    "    df_filtered = df_model.dropna(subset=[target_col]).copy()\n",
    "    \n",
    "    # 1. Identify feature lists\n",
    "    actual_cat_cols = [col for col in global_cat_cols if col in df_filtered.columns]\n",
    "    actual_num_cols = [col for col in feature_cols if col not in actual_cat_cols and col in df_filtered.columns] \n",
    "    \n",
    "    # 2. Enforce feature order\n",
    "    X = df_filtered[actual_num_cols + actual_cat_cols].copy() \n",
    "    y = df_filtered[target_col]\n",
    "    \n",
    "    # 3. Imputation \n",
    "    for col in actual_num_cols:\n",
    "        X[col] = pd.to_numeric(X[col], errors='coerce').fillna(X[col].median())\n",
    "    for col in actual_cat_cols:\n",
    "        X[col] = X[col].fillna('missing') \n",
    "    \n",
    "    # 4. Split\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    relative_val_size = val_size / (train_size + val_size)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=relative_val_size, random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    print(f\"Data Loaded and Split (Actual Features: {len(actual_num_cols) + len(actual_cat_cols)}):\")\n",
    "    print(f\"  Training Set Size: {len(X_train)}\")\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, actual_num_cols, actual_cat_cols\n",
    "\n",
    "def define_preprocessor(numerical_features, categorical_features):\n",
    "    \"\"\"Defines the ColumnTransformer pipeline for scaling and encoding.\"\"\"\n",
    "    \n",
    "    numerical_pipeline = StandardScaler()\n",
    "    categorical_pipeline = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_pipeline, numerical_features),\n",
    "            ('cat', categorical_pipeline, categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "    return preprocessor\n",
    "\n",
    "def build_mlp_model(input_shape, hidden_layers, activation='relu', dropout_rate=0.2, learning_rate=0.001):\n",
    "    \"\"\"Creates a sequential Multi-Layer Perceptron (MLP) model for regression.\"\"\"\n",
    "    set_random_seeds(RANDOM_STATE)\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(hidden_layers[0], activation=activation, input_shape=(input_shape,),\n",
    "                    kernel_initializer=tf.keras.initializers.glorot_uniform(seed=RANDOM_STATE),\n",
    "                    bias_initializer='zeros'))\n",
    "    model.add(Dropout(dropout_rate, seed=RANDOM_STATE))\n",
    "    \n",
    "    for units in hidden_layers[1:]:\n",
    "        model.add(Dense(units, activation=activation,\n",
    "                       kernel_initializer=tf.keras.initializers.glorot_uniform(seed=RANDOM_STATE),\n",
    "                       bias_initializer='zeros'))\n",
    "        model.add(Dropout(dropout_rate, seed=RANDOM_STATE))\n",
    "        \n",
    "    model.add(Dense(1,\n",
    "                   kernel_initializer=tf.keras.initializers.glorot_uniform(seed=RANDOM_STATE),\n",
    "                   bias_initializer='zeros')) \n",
    "    \n",
    "    optimizer = tf.keras.optimizers.legacy.Adam(\n",
    "        learning_rate=learning_rate,\n",
    "        beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False\n",
    "    )\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae', tf.keras.metrics.R2Score()])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def run_hyperparameter_search(X_train, y_train, X_val, y_val, num_features, model_name):\n",
    "    \"\"\"Performs a custom grid search over predefined NN architecture and hyperparameters.\"\"\"\n",
    "    \n",
    "    layer_configs = [(256, 128, 64), (128, 64), (64, 32), (256, 128)]\n",
    "    learning_rates = [0.01, 0.001, 0.0001]\n",
    "    batch_sizes = [32, 64, 128]\n",
    "    dropout_rates = [0.1, 0.3]\n",
    "    \n",
    "    best_r2 = -float('inf')\n",
    "    best_params = {}\n",
    "    best_model = None\n",
    "    \n",
    "    print(f\"\\n--- Starting NN Grid Search for {model_name} ---\")\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "    total_runs = len(layer_configs) * len(learning_rates) * len(batch_sizes) * len(dropout_rates)\n",
    "    run_count = 0\n",
    "    \n",
    "    for layers in layer_configs:\n",
    "        for lr in learning_rates:\n",
    "            for bs in batch_sizes:\n",
    "                for drop in dropout_rates:\n",
    "                    run_count += 1\n",
    "                    \n",
    "                    tf.keras.backend.clear_session()\n",
    "                    set_random_seeds(RANDOM_STATE)\n",
    "                    \n",
    "                    model = build_mlp_model(input_shape=num_features, hidden_layers=layers, learning_rate=lr, dropout_rate=drop)\n",
    "                    \n",
    "                    model.fit(\n",
    "                        X_train, y_train, validation_data=(X_val, y_val), \n",
    "                        epochs=100, batch_size=bs, verbose=0, \n",
    "                        callbacks=[early_stopping, reduce_lr],\n",
    "                        shuffle=False\n",
    "                    )\n",
    "\n",
    "                    val_metrics = model.evaluate(X_val, y_val, verbose=0)\n",
    "                    val_r2 = val_metrics[-1]\n",
    "                    \n",
    "                    if val_r2 > best_r2:\n",
    "                        best_r2 = val_r2\n",
    "                        best_params = {'hidden_layer_sizes': layers, 'learning_rate': lr, 'batch_size': bs, 'dropout_rate': drop}\n",
    "                        best_model = model \n",
    "\n",
    "    print(\"\\n--- Grid Search Complete ---\")\n",
    "    return best_model, best_params, best_r2\n",
    "\n",
    "def evaluate_test_predictions(y_test_pred_scaled, y_test_true_scaled, y_scaler):\n",
    "    \"\"\"Calculates and returns standard regression metrics after inverse-transforming predictions.\"\"\"\n",
    "    \n",
    "    # Inverse transform predictions and true values back to original percentage scale\n",
    "    y_pred = y_scaler.inverse_transform(y_test_pred_scaled).flatten()\n",
    "    y_true = y_scaler.inverse_transform(y_test_true_scaled).flatten() \n",
    "\n",
    "    if y_pred.shape != y_true.shape:\n",
    "        print(f\"ERROR: Prediction shape ({y_pred.shape}) does not match true target shape ({y_true.shape}). Cannot evaluate.\")\n",
    "        return {'RMSE': np.nan, 'MAE': np.nan, 'R2': np.nan}\n",
    "\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    return {'RMSE': np.sqrt(mse), 'MAE': mae, 'R2': r2}\n",
    "\n",
    "def print_evaluation_summary(model_name, metrics, best_params):\n",
    "    \"\"\"Formats and prints the final model results.\"\"\"\n",
    "    print(f\"\\n--- {model_name} Test Set Evaluation ---\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Optimal Architecture: {best_params['hidden_layer_sizes']}, LR={best_params['learning_rate']}, Drop={best_params['dropout_rate']}\")\n",
    "    \n",
    "    # Adjusted print format for DOLLAR value prediction\n",
    "    print(f\"Metric:              Value (in Original Dollar Scale)\")\n",
    "    print(f\"--------------------------------------------------\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {metrics['RMSE']:.2f}\") \n",
    "    print(f\"Mean Absolute Error (MAE):     {metrics['MAE']:.2f}\")\n",
    "    print(f\"Coefficient of Determination (R²): {metrics['R2']:.4f}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "# =========================================================================\n",
    "# 4. EXECUTION FLOW\n",
    "# =========================================================================\n",
    "\n",
    "def execute_model_run(df_input, num_cols, cat_cols, model_name):\n",
    "    \"\"\"Executes the full preprocessing, training, and evaluation for a given feature subset.\"\"\"\n",
    "    \n",
    "    print(f\"\\n\\n=======================================================\")\n",
    "    print(f\"MODEL RUN: {model_name}\")\n",
    "    print(f\"=======================================================\")\n",
    "    \n",
    "    # --- 1. Load and Split Data ---\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test, actual_num_cols, actual_cat_cols = load_and_split_data(\n",
    "        df_input, TARGET_COLUMN, num_cols, cat_cols, TRAIN_SIZE, VAL_SIZE, TEST_SIZE\n",
    "    )\n",
    "    \n",
    "    # --- 2. Target Scaling ---\n",
    "    y_scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "    y_train_scaled = y_scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
    "    y_val_scaled = y_scaler.transform(y_val.values.reshape(-1, 1))\n",
    "    y_test_scaled = y_scaler.transform(y_test.values.reshape(-1, 1))\n",
    "    \n",
    "    # --- 3. Preprocessing Pipeline ---\n",
    "    preprocessor = define_preprocessor(actual_num_cols, actual_cat_cols)\n",
    "    \n",
    "    print(f\"Fitting and transforming features (Numeric: {len(actual_num_cols)}, Categorical: {len(actual_cat_cols)})...\")\n",
    "    X_train_proc = preprocessor.fit_transform(X_train)\n",
    "    X_val_proc = preprocessor.transform(X_val)\n",
    "    X_test_proc = preprocessor.transform(X_test)\n",
    "    \n",
    "    # --- 4. Type Conversion for TensorFlow ---\n",
    "    X_train_proc = X_train_proc.astype(np.float32)\n",
    "    X_val_proc = X_val_proc.astype(np.float32)\n",
    "    X_test_proc = X_test_proc.astype(np.float32)\n",
    "    \n",
    "    y_train_proc = y_train_scaled.astype(np.float32)\n",
    "    y_val_proc = y_val_scaled.astype(np.float32)\n",
    "    y_test_proc = y_test_scaled.astype(np.float32) \n",
    "    \n",
    "    num_features = X_train_proc.shape[1]\n",
    "    print(f\"Features (Numeric+Encoded): {num_features}\")\n",
    "    \n",
    "    # --- 5. Hyperparameter Search and Training ---\n",
    "    best_model, best_params, best_val_r2 = run_hyperparameter_search(\n",
    "        X_train_proc, y_train_proc, X_val_proc, y_val_proc, num_features, model_name\n",
    "    )\n",
    "\n",
    "    # --- 6. Final Evaluation on Test Set ---\n",
    "    if best_model:\n",
    "        y_test_pred_scaled = best_model.predict(X_test_proc, verbose=0)\n",
    "        \n",
    "        test_metrics = evaluate_test_predictions(y_test_pred_scaled, y_test_proc, y_scaler)\n",
    "        \n",
    "        print_evaluation_summary(model_name, test_metrics, best_params)\n",
    "        \n",
    "        return best_model, best_params, test_metrics\n",
    "    else:\n",
    "        print(f\"ERROR: {model_name} failed to find a best model.\")\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to perform all data preparation and run both models.\"\"\"\n",
    "    \n",
    "    # --- STEP 0: INITIAL DATA LOAD AND CLEANING ---\n",
    "    df_raw = pd.read_csv(DATASET_FILE)\n",
    "    \n",
    "    # --- Handle Duplicates and Target/Population Setup ---\n",
    "    merge_key_col = [col for col in df_raw.columns if 'Merge_Key' in col][0]\n",
    "    pop_col = [col for col in df_raw.columns if re.search(r'Total_Population', col, re.IGNORECASE)][0]\n",
    "    \n",
    "    df_raw[pop_col] = pd.to_numeric(df_raw[pop_col], errors='coerce')\n",
    "    df_base_cleaned = df_raw.sort_values(by=pop_col, ascending=False).drop_duplicates(subset=[merge_key_col], keep='first').copy()\n",
    "    print(f\"Initial Duplicates Removed: {len(df_raw) - len(df_base_cleaned)} rows dropped (Base Data).\")\n",
    "\n",
    "    # --- ADD GEOGRAPHIC REGION ---\n",
    "    df_base_cleaned = add_us_regions(df_base_cleaned)\n",
    "    \n",
    "    # --- DROP LEAKY COLUMNS ---\n",
    "    # Drop all housing price columns to prevent leakage\n",
    "    df_base_cleaned = df_base_cleaned.drop(columns=[\n",
    "        'Median_Housing_Value_2021', 'Median_Housing_Value_2022'# , 'Median_Housing_Value_2023'\n",
    "    ], errors='ignore').copy()\n",
    "\n",
    "    # --- STEP 1: RUN NON-VIBE BASE MODEL ---\n",
    "    \n",
    "    # Feature set for Non-VIBE: BASE_NON_VIBE_COLS (Numerical) + region (Categorical)\n",
    "    BASE_NON_VIBE_COLS_FINAL = BASE_NON_VIBE_COLS + ['region']\n",
    "    \n",
    "    df_model_non_vibe = df_base_cleaned[BASE_NON_VIBE_COLS_FINAL + [TARGET_COLUMN]].copy()\n",
    "    \n",
    "    # Execute Baseline Run\n",
    "    model_non_vibe, params_non_vibe, metrics_non_vibe = execute_model_run(\n",
    "        df_model_non_vibe, BASE_NON_VIBE_COLS, ['region'], \"NON-VIBE BASE MODEL\"\n",
    "    )\n",
    "    \n",
    "    # --- STEP 2: RUN FULL VIBE MODEL ---\n",
    "    \n",
    "    # Vibe model features: BASE_NON_VIBE_COLS + ALL VIBE COLS (including categoricals)\n",
    "    FULL_VIBE_COLS_FINAL = BASE_NON_VIBE_COLS + VIBE_COLS_ALL + ['region']\n",
    "    \n",
    "    # Numerical features for VIBE model: BASE_NON_VIBE_COLS + VIBE numerical features\n",
    "    VIBE_NUM_COLS = [col for col in VIBE_COLS_ALL if col not in CATEGORICAL_COLS]\n",
    "    FULL_NUM_COLS = BASE_NON_VIBE_COLS + VIBE_NUM_COLS\n",
    "    \n",
    "    # Categorical features for VIBE model: water_0... + region\n",
    "    FULL_CAT_COLS = CATEGORICAL_COLS\n",
    "    \n",
    "    df_model_vibe = df_base_cleaned[FULL_VIBE_COLS_FINAL + [TARGET_COLUMN]].copy()\n",
    "    \n",
    "    # Execute VIBE Run\n",
    "    model_vibe, params_vibe, metrics_vibe = execute_model_run(\n",
    "        df_model_vibe, FULL_NUM_COLS, FULL_CAT_COLS, \"FULL VIBE MODEL\"\n",
    "    )\n",
    "    \n",
    "    # --- 3. FINAL PROJECT COMPARISON ---\n",
    "    \n",
    "    if metrics_non_vibe and metrics_vibe:\n",
    "        print(\"\\n\\n######################################################\")\n",
    "        print(\"####### FINAL VIBE FEATURE IMPACT COMPARISON #########\")\n",
    "        print(\"######################################################\")\n",
    "        \n",
    "        r2_non_vibe = metrics_non_vibe['R2']\n",
    "        r2_vibe = metrics_vibe['R2']\n",
    "        \n",
    "        if r2_vibe > r2_non_vibe:\n",
    "            improvement = (r2_vibe - r2_non_vibe) / r2_non_vibe\n",
    "            print(f\"CONCLUSION: VIBE Features IMPROVED Predictive Power.\")\n",
    "            print(f\"Baseline R² (Non-VIBE): {r2_non_vibe:.4f}\")\n",
    "            print(f\"VIBE Model R²:          {r2_vibe:.4f} (+{improvement:.1%})\")\n",
    "        else:\n",
    "            if abs(r2_vibe - r2_non_vibe) < 0.005: \n",
    "                print(f\"CONCLUSION: VIBE Features had NO significant impact on R².\")\n",
    "            else:\n",
    "                print(f\"CONCLUSION: VIBE Features DEGRADED Predictive Power.\")\n",
    "            print(f\"Baseline R² (Non-VIBE): {r2_non_vibe:.4f}\")\n",
    "            print(f\"VIBE Model R²:          {r2_vibe:.4f}\")\n",
    "            \n",
    "        print(\"\\nRMSE Comparison (in Dollars):\")\n",
    "        print(f\"Baseline RMSE: ${metrics_non_vibe['RMSE']:.2f}\")\n",
    "        print(f\"VIBE Model RMSE: ${metrics_vibe['RMSE']:.2f}\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tf.keras.backend.clear_session()\n",
    "    set_random_seeds(RANDOM_STATE)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f79157-e290-40cc-8624-ebddf6bca7ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf215)",
   "language": "python",
   "name": "tf215"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
